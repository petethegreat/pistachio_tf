{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e67e146c-7741-4e5d-b3e5-152d1fcce01c",
   "metadata": {},
   "source": [
    "# tensorflow pistachio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9645c12f-836e-43ef-a516-dd6134687a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 04:27:59.446787: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c077c-56e8-4081-b5dc-34945bd1fa61",
   "metadata": {},
   "source": [
    "## arff to csv\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19e49d4-8185-4a92-b60f-7e94609fccca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/pistachio_16.csv exists\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from scipy.io import arff\n",
    "import os \n",
    "label_mapping = {'Kirmizi_Pistachio': 0, 'Siit_Pistachio': 1}\n",
    "\n",
    "def load_arff_file(input_arff: str) -> pd.DataFrame:\n",
    "    \"\"\"convert arff file to parquet\"\"\"\n",
    "    if not os.path.exists(input_arff):\n",
    "        raise ValueError(f\"input file '{input_arff}' does not exist\")\n",
    "    print(f'loading arff file {input_arff}')\n",
    "    data, meta = arff.loadarff(input_arff)\n",
    "    print(f\"arff metadata: {meta}\")\n",
    "    df = pd.DataFrame(data)\n",
    "    df['Class'] = df['Class'].astype(str).map(label_mapping)\n",
    "    \n",
    "    return df\n",
    "##################\n",
    "\n",
    "arff_filename = './data/Pistachio_16_Features_Dataset.arff'\n",
    "csv_filename = './data/pistachio_16.csv'\n",
    "if not os.path.exists(csv_filename):\n",
    "    df = load_arff_file(arff_filename)\n",
    "    df.head()\n",
    "    df.to_csv(csv_filename, index=False, header=True)\n",
    "    print(f'wrote file to {csv_filename}')\n",
    "else:\n",
    "    print(f'{csv_filename} exists')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e66103-32dc-4820-8089-71b0f3b4b789",
   "metadata": {},
   "source": [
    "## dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7845cd-40fe-4406-9874-ca68bc501561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "df shape = (1374, 17)\n",
      "   Class  AREA\n",
      "0      0   801\n",
      "1      1   573\n",
      "validation\n",
      "df shape = (344, 17)\n",
      "   Class  AREA\n",
      "0      0   204\n",
      "1      1   140\n",
      "test\n",
      "df shape = (430, 17)\n",
      "   Class  AREA\n",
      "0      0   227\n",
      "1      1   203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AREA',\n",
       " 'PERIMETER',\n",
       " 'MAJOR_AXIS',\n",
       " 'MINOR_AXIS',\n",
       " 'ECCENTRICITY',\n",
       " 'EQDIASQ',\n",
       " 'SOLIDITY',\n",
       " 'CONVEX_AREA',\n",
       " 'EXTENT',\n",
       " 'ASPECT_RATIO',\n",
       " 'ROUNDNESS',\n",
       " 'COMPACTNESS',\n",
       " 'SHAPEFACTOR_1',\n",
       " 'SHAPEFACTOR_2',\n",
       " 'SHAPEFACTOR_3',\n",
       " 'SHAPEFACTOR_4']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_csv_data(infilename: str, train_filename: str, test_filename:str, test_fraction: float):\n",
    "    df = pd.read_csv(infilename, header=0)\n",
    "    columns = df.columns\n",
    "    df['split_var'] = np.random.uniform(size=len(df))\n",
    "    train_df = df.loc[df.split_var <= test_fraction][columns]\n",
    "    test_df = df.loc[df.split_var > test_fraction][columns]\n",
    "    train_df.to_csv(train_filename, index=False, header=True)\n",
    "    test_df.to_csv(test_filename, index=False, header=True)\n",
    "    print(f'wrote {len(train_df)} records to {train_filename}')\n",
    "    print(f'wrote {len(test_df)} records to {test_filename}')\n",
    "\n",
    "def df_to_dataset(df: pd.DataFrame, target_column: str):\n",
    "    feature_df = df.copy()\n",
    "    target = feature_df.pop(target_column)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(feature_df), target))\n",
    "    dataset = dataset.shuffle(buffer_size=len(feature_df))\\\n",
    "        .batch(2)\\\n",
    "        .prefetch(2)\n",
    "    return dataset\n",
    "\n",
    "def split_data_to_frames(infilename: str, test_fraction: float=0.2, val_fraction: float=0.2, seed:int=43):\n",
    "    \"\"\" load csv as dataframe, split\"\"\"\n",
    "\n",
    "    df = pd.read_csv(infilename)\n",
    "    train_val_df, test_df = train_test_split(df, test_size=test_fraction, random_state=seed)\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=val_fraction, random_state=seed+1)\n",
    "\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, valid_df, test_df = split_data_to_frames(csv_filename)\n",
    "\n",
    "for setname, df in zip(['train','validation','test'],[train_df, valid_df, test_df]):\n",
    "    print(setname)\n",
    "    print(f'df shape = {df.shape}')\n",
    "    agged = df.groupby('Class').agg({'AREA':'count'}).reset_index()\n",
    "    print(agged)\n",
    "\n",
    "\n",
    "feature_columns = list(train_df.columns)\n",
    "feature_columns.remove('Class')\n",
    "feature_columns\n",
    "\n",
    "# train_ds, valid_ds, test_ds = get_datasets(csv_filename, 'Class')\n",
    "\n",
    "#     train_ds = df_to_dataset(train_df, target_column)\n",
    "#     val_ds = df_to_dataset(val_df, target_column)\n",
    "#     test_ds = df_to_dataset(test_df, target_column)\n",
    "\n",
    "\n",
    "    \n",
    "# train_filename = './data/pistachio_train.csv'\n",
    "# test_filename = './data/pistachio_test.csv'\n",
    "\n",
    "# if not (os.path.exists(train_filename) and os.path.exists(test_filename)):\n",
    "#     split_csv_data(csv_filename, train_filename, test_filename, 0.2)\n",
    "# else:\n",
    "#     print(f'{train_filename} and {test_filename} exist')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778ff87f-3a47-493f-8bdc-54309fc32215",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = df_to_dataset(train_df,'Class')\n",
    "valid_ds = df_to_dataset(valid_df,'Class')\n",
    "test_ds = df_to_dataset(test_df,'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0033c4dd-2a95-4b3c-b256-cdbb478cf601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: {'0': 801, '1': 573, 'proportion_0': 0.5829694323144105, 'proportion_1': 0.4170305676855895}\n",
      "valid: {'0': 204, '1': 140, 'proportion_0': 0.5930232558139535, 'proportion_1': 0.4069767441860465}\n",
      "test: {'0': 227, '1': 203, 'proportion_0': 0.5279069767441861, 'proportion_1': 0.4720930232558139}\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_class_proportions(the_dataset):\n",
    "    def count_class(counts, batch, num_classes=2):\n",
    "        labels = batch[1] # class is second element of tuple\n",
    "        for i in range(num_classes):\n",
    "            cc = tf.cast(labels == i, tf.int32)\n",
    "            counts[str(i)] += tf.reduce_sum(cc)\n",
    "        return counts\n",
    "    initial_state = {'0':0, '1':0}\n",
    "    proportions = {k: v.numpy() for k,v in the_dataset.reduce(reduce_func=count_class, initial_state=initial_state).items()}\n",
    "    total = sum(proportions.values())\n",
    "    proportions.update({f'proportion_{k}': v/total for k,v in proportions.items()})\n",
    "    \n",
    "    return proportions\n",
    "    \n",
    "\n",
    "print(f'train: {get_dataset_class_proportions(train_ds)}')\n",
    "print(f'valid: {get_dataset_class_proportions(valid_ds)}')\n",
    "print(f'test: {get_dataset_class_proportions(test_ds)}')\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1902bad6-0b5a-4cbf-8163-159c21a7b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_func(features, labels):\n",
    "#     return tf.transpose(tf.stack([features[k] for k in features])), tf.reshape(labels,[-1,1])\n",
    "\n",
    "# # use dataset.map to concatenate feature dictionary into tensor\n",
    "# pistachio_train_batches = tf.data.experimental.make_csv_dataset(\n",
    "#     train_filename, batch_size=4,\n",
    "#     num_epochs=1,\n",
    "#     label_name=\"Class\").map(map_func)\n",
    "# pistachio_test_batches = tf.data.experimental.make_csv_dataset(\n",
    "#     test_filename, batch_size=4,\n",
    "#     num_epochs=1,\n",
    "#     label_name=\"Class\").map(map_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a8ac90-eceb-44cb-a73f-8d55a76c66be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'label': [1 0]\n",
      "features batch shape: (2,)\n",
      "'label': [0 0]\n",
      "features batch shape: (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 04:28:14.714007: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "batch = 0\n",
    "# for feature_batch, label_batch in pistachio_train_batches.take(2):\n",
    "for feature_batch, label_batch in train_ds.take(2):\n",
    "    # print(f'{batch}, {label_batch.shape}')\n",
    "    # cat_batch = tf.stack([feature_batch['AREA'],feature_batch['PERIMETER']],axis=1)\n",
    "    # cat_batch = tf.stack([feature_batch[k] for k in feature_batch],axis=1)\n",
    "\n",
    "    # batch += 1\n",
    "    \n",
    "    print(\"'label': {}\".format(label_batch))\n",
    "    # print(cat_batch)\n",
    "    print(f\"features batch shape: {feature_batch['AREA'].shape}\")\n",
    "    # print(feature_batch.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3197b-18b2-4f33-b760-ab2c45de50fd",
   "metadata": {},
   "source": [
    "## Functional API\n",
    "try this instead, dataset is weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386d0672-0aab-42b6-8cdd-e1bdab5388c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.cardinality().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4379442-c952-4857-9fea-270f6f86e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Normalization\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.metrics import Accuracy, AUC, Recall, Precision\n",
    "\n",
    "def get_pistachio_model(feature_columns: List[str], train_dataset: tf.data.Dataset, units: int=10):\n",
    "    \"\"\"build a pistachio model using functional api\"\"\"\n",
    "    def _get_feature_normalizers():\n",
    "        \"\"\"initialise and adapt the feature normalisers\"\"\"\n",
    "        print(f'preprocessing - initialising normalisers')\n",
    "        normalizers = {}\n",
    "        for feature in feature_columns:\n",
    "            normaliser =  Normalization(axis=None, name=f'normalizer_{feature}')\n",
    "            just_this_feature_ds = train_dataset.map(lambda x,y: x[feature])\n",
    "            normaliser.adapt(just_this_feature_ds)\n",
    "            normalizers[feature] = normaliser\n",
    "        return normalizers\n",
    "        \n",
    "    def _build_model(normalizers: Dict):\n",
    "        normalized_inputs = []\n",
    "        raw_inputs = []\n",
    "        for feature in feature_columns:\n",
    "            feature_input = tf.keras.Input(shape=(1,), name=feature)\n",
    "            raw_inputs.append(feature_input)\n",
    "            normalized_input = normalizers[feature](feature_input)\n",
    "            normalized_inputs.append(normalized_input)\n",
    "\n",
    "        input_layer = tf.keras.layers.concatenate(normalized_inputs)\n",
    "\n",
    "        # densely connected layers\n",
    "        d1 = Dense(units, activation='relu', name='dense_1')\n",
    "        d2 = Dense(units, activation='relu', name='dense_2')\n",
    "\n",
    "        # output layer\n",
    "        output_layer = Dense(1, activation='sigmoid', name='output')\n",
    "\n",
    "        # define graph\n",
    "        x = d1(input_layer)\n",
    "        x = d2(x)\n",
    "        final_output = output_layer(x)\n",
    "        model = tf.keras.Model(raw_inputs, final_output)\n",
    "        return model\n",
    "    normalizers = _get_feature_normalizers()\n",
    "    model = _build_model(normalizers)\n",
    "    return model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e903376-6aee-4746-acac-02a597bacccf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 0.01}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m      5\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAccuracy(),\n\u001b[1;32m      7\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAUC(),\n\u001b[1;32m      8\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mPrecision(),\n\u001b[1;32m      9\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mRecall()]\n\u001b[1;32m     11\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(logdir, update_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     13\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m) \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# tf.keras.callbacks.LearningRateScheduler(lambda epoch, lr: max(lr*0.9, 1e-3))\u001b[39;00m\n\u001b[1;32m     15\u001b[0m ]\n\u001b[0;32m---> 17\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# build the model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m get_pistachio_model(feature_columns, train_ds)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/adam.py:60\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, name, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     45\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     59\u001b[0m ):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1 \u001b[38;5;241m=\u001b[39m beta_1\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2 \u001b[38;5;241m=\u001b[39m beta_2\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/optimizer.py:19\u001b[0m, in \u001b[0;36mTFOptimizer.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:38\u001b[0m, in \u001b[0;36mBaseOptimizer.__init__\u001b[0;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `decay` is no longer supported and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument(s) not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     name \u001b[38;5;241m=\u001b[39m auto_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Argument(s) not recognized: {'lr': 0.01}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "logdir = './pistachio_model_logs'\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.Accuracy(),\n",
    "    tf.keras.metrics.AUC(),\n",
    "    tf.keras.metrics.Precision(),\n",
    "    tf.keras.metrics.Recall()]\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(logdir, update_freq='batch'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.001) \n",
    "    # tf.keras.callbacks.LearningRateScheduler(lambda epoch, lr: max(lr*0.9, 1e-3))\n",
    "]\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# build the model\n",
    "model = get_pistachio_model(feature_columns, train_ds)\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145f6be3-fcc6-4df0-9c54-d5271db7a5e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mplot_model(\u001b[43mmodel\u001b[49m, show_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rankdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c51cd6a6-1960-43ba-b2a5-2f61b9ba587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py:669: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 985us/step - accuracy: 0.0000e+00 - auc_4: 0.7439 - loss: 0.5602 - precision_4: 0.6182 - recall_4: 0.7557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x77dabc5f66d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48becd0f-e403-4e38-8de1-0cf00d1dea89",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd86bdb2-b2a3-47ae-b4af-24a4ce24404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Normalization\n",
    "# from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class PistachioModel(Model):\n",
    "    def __init__(self, units: int=10):\n",
    "        super().__init__()\n",
    "        self._units = units\n",
    "        self.normalizers = {}\n",
    "    \n",
    "    def define_normalizers(self, train_dataset, feature_columns):\n",
    "        '''define normalizers based on keys in feature dataset, and adapt them'''\n",
    "        for k in feature_columns:\n",
    "            self.normalizers[k] = Normalization(axis=None, name=f'normalizer_{k}')\n",
    "            just_this_feature_ds = train_dataset.map(lambda x,y: x[k])\n",
    "            self.normalizers[k].adapt(just_this_feature_ds)    \n",
    "\n",
    "    def build(self, features):\n",
    "\n",
    "        # raise error if no normalizers defined\n",
    "        if (len(self.normalizers) == 0):\n",
    "            raise ValueError('cannot build until normalizers defined')\n",
    "\n",
    "\n",
    "        self.normalized_inputs = []\n",
    "\n",
    "        for k in features:\n",
    "            feature_input = tf.keras.Input(shape=(1,), name=f'raw_{k}')\n",
    "            # normed_input = self.normalizers[k](feature_input)\n",
    "            # self.normalized_inputs.append(normed_input)\n",
    "            \n",
    "        # self.all_inputs = tf.layers.concatenate(name='all_normed_inputs')\n",
    "        \n",
    "        self.d1 = Dense(self._units, activation='relu', name='dense_1')\n",
    "        self.d2 = Dense(self._units, activation='relu', name='dense_2')\n",
    "        self.lout = Dense(1, activation='sigmoid', name='output')\n",
    "        \n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        normed_inputs = []\n",
    "        for k in feature_columns:\n",
    "            normed_k = self.normalizers[k](x)\n",
    "            normed_inputs.append(normed_k)\n",
    "        all_normed_inputs = tf.layers.concatenate(normed_inputs)\n",
    "          \n",
    "        x = self.d1(all_normed_inputs)\n",
    "        x = self.d2(x)\n",
    "        return self.lout(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = PistachioModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b95271b5-62c5-4dd1-9345-bf7df1413e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 03:27:27.824160: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:27.895836: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:27.955659: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:28.017901: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:28.077613: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:28.138449: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:28.205840: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:28.271555: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:28.362145: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:28.423163: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:28.490890: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:28.555599: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:28.617822: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:28.681493: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:28.741527: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-11-24 03:27:28.806951: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.define_normalizers(train_ds, feature_columns)\n",
    "model.build(feature_columns)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea6012-3ffa-4ca8-b617-9d0999e588a4",
   "metadata": {},
   "source": [
    "## Keras model.fit api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "afb24047-86ea-4f70-939c-769ca8089d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy', 'auc'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80e71dc3-4e8f-4895-946b-04af6afcea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling Normalization.call().\n\n\u001b[1mExpected float32, but got Tensor(\"Cast:0\", shape=(None,), dtype=float32) of type 'SymbolicTensor'.\u001b[0m\n\nArguments received by Normalization.call():\n  • inputs={'AREA': 'tf.Tensor(shape=(None,), dtype=float32)', 'PERIMETER': 'tf.Tensor(shape=(None,), dtype=float32)', 'MAJOR_AXIS': 'tf.Tensor(shape=(None,), dtype=float32)', 'MINOR_AXIS': 'tf.Tensor(shape=(None,), dtype=float32)', 'ECCENTRICITY': 'tf.Tensor(shape=(None,), dtype=float32)', 'EQDIASQ': 'tf.Tensor(shape=(None,), dtype=float32)', 'SOLIDITY': 'tf.Tensor(shape=(None,), dtype=float32)', 'CONVEX_AREA': 'tf.Tensor(shape=(None,), dtype=float32)', 'EXTENT': 'tf.Tensor(shape=(None,), dtype=float32)', 'ASPECT_RATIO': 'tf.Tensor(shape=(None,), dtype=float32)', 'ROUNDNESS': 'tf.Tensor(shape=(None,), dtype=float32)', 'COMPACTNESS': 'tf.Tensor(shape=(None,), dtype=float32)', 'SHAPEFACTOR_1': 'tf.Tensor(shape=(None,), dtype=float32)', 'SHAPEFACTOR_2': 'tf.Tensor(shape=(None,), dtype=float32)', 'SHAPEFACTOR_3': 'tf.Tensor(shape=(None,), dtype=float32)', 'SHAPEFACTOR_4': 'tf.Tensor(shape=(None,), dtype=float32)'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[48], line 43\u001b[0m, in \u001b[0;36mPistachioModel.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m normed_inputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m feature_columns:\n\u001b[0;32m---> 43\u001b[0m     normed_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     normed_inputs\u001b[38;5;241m.\u001b[39mappend(normed_k)\n\u001b[1;32m     45\u001b[0m all_normed_inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mconcatenate(normed_inputs)\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling Normalization.call().\n\n\u001b[1mExpected float32, but got Tensor(\"Cast:0\", shape=(None,), dtype=float32) of type 'SymbolicTensor'.\u001b[0m\n\nArguments received by Normalization.call():\n  • inputs={'AREA': 'tf.Tensor(shape=(None,), dtype=float32)', 'PERIMETER': 'tf.Tensor(shape=(None,), dtype=float32)', 'MAJOR_AXIS': 'tf.Tensor(shape=(None,), dtype=float32)', 'MINOR_AXIS': 'tf.Tensor(shape=(None,), dtype=float32)', 'ECCENTRICITY': 'tf.Tensor(shape=(None,), dtype=float32)', 'EQDIASQ': 'tf.Tensor(shape=(None,), dtype=float32)', 'SOLIDITY': 'tf.Tensor(shape=(None,), dtype=float32)', 'CONVEX_AREA': 'tf.Tensor(shape=(None,), dtype=float32)', 'EXTENT': 'tf.Tensor(shape=(None,), dtype=float32)', 'ASPECT_RATIO': 'tf.Tensor(shape=(None,), dtype=float32)', 'ROUNDNESS': 'tf.Tensor(shape=(None,), dtype=float32)', 'COMPACTNESS': 'tf.Tensor(shape=(None,), dtype=float32)', 'SHAPEFACTOR_1': 'tf.Tensor(shape=(None,), dtype=float32)', 'SHAPEFACTOR_2': 'tf.Tensor(shape=(None,), dtype=float32)', 'SHAPEFACTOR_3': 'tf.Tensor(shape=(None,), dtype=float32)', 'SHAPEFACTOR_4': 'tf.Tensor(shape=(None,), dtype=float32)'}"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6196f01d-4e1d-4014-9041-a8228d2117e3",
   "metadata": {},
   "source": [
    "## sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ffdfcd8e-0ee0-4882-a93c-3cef10012d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.BatchNormalization(), \n",
    "  tf.keras.layers.Dense(16, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(16),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model2.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy', 'auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32a9cc68-ae2c-4564-af0a-16be2692cbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - accuracy: 0.7284 - auc: 0.8170 - loss: 0.5204\n",
      "Epoch 2/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - accuracy: 0.7720 - auc: 0.8719 - loss: 0.4395\n",
      "Epoch 3/10\n",
      "\u001b[1m  1/111\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.7500 - auc: 1.0000 - loss: 0.2692"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 02:52:53.956163: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-04-30 02:52:54.084876: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - accuracy: 0.8066 - auc: 0.8861 - loss: 0.4293\n",
      "Epoch 4/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - accuracy: 0.7972 - auc: 0.8884 - loss: 0.4236\n",
      "Epoch 5/10\n",
      "\u001b[1m  1/111\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2399"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 02:52:54.215464: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-04-30 02:52:54.346195: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8084 - auc: 0.8767 - loss: 0.4463  \n",
      "Epoch 6/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - accuracy: 0.7599 - auc: 0.8354 - loss: 0.5226 \n",
      "Epoch 7/10\n",
      "\u001b[1m  1/111\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.7500 - auc: 0.6667 - loss: 0.5059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 02:52:54.488375: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-04-30 02:52:54.622119: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - accuracy: 0.7879 - auc: 0.8461 - loss: 0.4867\n",
      "Epoch 8/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - accuracy: 0.8098 - auc: 0.8833 - loss: 0.4322\n",
      "Epoch 9/10\n",
      "\u001b[1m  1/111\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2679"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 02:52:54.760318: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-04-30 02:52:54.891889: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.7919 - auc: 0.8692 - loss: 0.4556\n",
      "Epoch 10/10\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.7603 - auc: 0.8445 - loss: 0.4759 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 02:52:55.023937: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-04-30 02:52:55.158823: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f8e2ac1f350>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(pistachio_train_batches, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd47e368-7596-4cbd-a224-9aeb9879c0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427/427 - 0s - 1ms/step - accuracy: 0.8639 - auc: 0.9313 - loss: 0.3545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 02:54:06.293289: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35447990894317627, 0.8639296293258667, 0.9313317537307739]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(pistachio_test_batches,verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "84046f88-2c10-4a9d-ad6a-da772e906c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted prob: [0.8645645], label: [1]\n",
      "predicted prob: [0.31095818], label: [0]\n",
      "predicted prob: [0.94427866], label: [1]\n",
      "predicted prob: [0.00063194], label: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 03:04:05.021128: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for features, labels in pistachio_test_batches.take(1):\n",
    "    predictions = model2(features)\n",
    "    for p,l in zip(predictions, labels):\n",
    "        print(f'predicted prob: {p}, label: {l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630f2c6-e791-422d-bc5f-b6eb964590c7",
   "metadata": {},
   "source": [
    "## custom training loop stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ece09-1097-4802-a799-ac58f4b42f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=True)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a43b7-7b52-4502-b0e9-5e6415cb1b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  predictions = model(images, training=False)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b049d-3213-460c-ac02-d02a01454360",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_state()\n",
    "  train_accuracy.reset_state()\n",
    "  test_loss.reset_state()\n",
    "  test_accuracy.reset_state()\n",
    "\n",
    "  for images, labels in train_ds:\n",
    "    train_step(images, labels)\n",
    "\n",
    "  for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "  print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result():0.2f}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100:0.2f}, '\n",
    "    f'Test Loss: {test_loss.result():0.2f}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100:0.2f}'\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
